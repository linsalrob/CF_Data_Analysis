{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86a09674-cb56-46b9-9c5f-ed992a8fe9c6",
   "metadata": {},
   "source": [
    "# Autoencoding our variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f46fab-a83b-4dc1-a303-d2cc454073b5",
   "metadata": {},
   "source": [
    "### Import the python torch and tensorflow modules and make sure we're ready to rock and roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48a18147-ab48-4201-abc3-fa21b345f38a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers, models\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"FATAL: Please run this notebook in an environment with CUDA available\", file=sys.stderr)\n",
    "else:\n",
    "    print(\"Lets go GPU\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bfd2f1-7492-4816-aef7-dfa246fad2de",
   "metadata": {},
   "source": [
    "## Import the libraries.\n",
    "\n",
    "This is my standard import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ff4468-f4ea-4b19-ae78-28a75ca708a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from socket import gethostname\n",
    "\n",
    "hostname = gethostname()\n",
    "\n",
    "if hostname.startswith('hpc-node'):\n",
    "    IN_DEEPTHOUGHT = True\n",
    "    sys.path.append('..')\n",
    "else:\n",
    "    IN_DEEPTHOUGHT = False\n",
    "from cf_analysis_lib.load_libraries import *\n",
    "import cf_analysis_lib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e530cbc-d44d-4a97-855b-4238c679a95d",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74b15ee-d09d-41ba-bbad-c707eb7703c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_type = \"MGI\"\n",
    "datadir = '..'\n",
    "sslevel = 'subsystems_norm_ss.tsv.gz'\n",
    "taxa = \"family\"\n",
    "\n",
    "df, metadata = cf_analysis_lib.read_the_data(sequence_type, datadir, sslevel='subsystems_norm_ss.tsv.gz', taxa=\"family\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb923b7-e0b4-44eb-829d-bb9054195dbe",
   "metadata": {},
   "source": [
    "## Somewhere to save the outputs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fbdcfe-4be9-4cb3-89c5-a894096350bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = 'cluster_gbrfs'\n",
    "os.makedirs(outdir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35755fd-917c-4875-96f9-f83df6830524",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00562fd-0939-47c8-b3a1-f83057b97e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validate_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "print(f\"Train: {training_data.shape}\")\n",
    "print(f\"Test: {validate_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024d4b34-b09d-4a0c-9686-7bc24972c2b5",
   "metadata": {},
   "source": [
    "## Develop the autoencoder\n",
    "\n",
    "Because an autoencoder reconstructs its inputs, the “labels” (y) are the same as the features (X), so `x=training_data`, and `y=training_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeee3d5f-0a58-4f06-a7c0-b4368a5cb1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do we have a saved model?\n",
    "\n",
    "if os.path.exists(os.path.join(outdir, \"autoencoder_model\")):\n",
    "    print(f\"Loading the autoencoder from disk. Remove {os.path.join(outdir, 'autoencoder_model')} to recreate the model\", file=sys.stderr)\n",
    "    autoencoder = models.load_model(os.path.join(outdir, \"autoencoder_model\"), compile=False)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    encoder = models.load_model(os.path.join(outdir, \"encoder_model\"), compile=False)\n",
    "    decoder = models.load_model(os.path.join(outdir, \"decoder_model\"), compile=False)\n",
    "    encoder.compile()\n",
    "    decoder.compile()\n",
    "else:\n",
    "    # 1) Define network parameters\n",
    "    input_dim = len(df.columns)\n",
    "    latent_dim = 50  # tweak this\n",
    "    \n",
    "    # 2) Build the encoder\n",
    "    encoder_input = tf.keras.Input(shape=(input_dim,))\n",
    "    x = layers.Dense(512, activation='relu')(encoder_input)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    encoder_output = layers.Dense(latent_dim, activation='linear')(x)\n",
    "    \n",
    "    # 3) Build the decoder\n",
    "    decoder_input = layers.Input(shape=(latent_dim,))\n",
    "    y = layers.Dense(128, activation='relu')(decoder_input)\n",
    "    y = layers.Dense(512, activation='relu')(y)\n",
    "    decoder_output = layers.Dense(input_dim, activation='linear')(y)\n",
    "    \n",
    "    # 4) Create autoencoder\n",
    "    encoder = tf.keras.Model(encoder_input, encoder_output, name=\"encoder\")\n",
    "    decoder = tf.keras.Model(decoder_input, decoder_output, name=\"decoder\")\n",
    "    \n",
    "    autoencoder_input = tf.keras.Input(shape=(input_dim,))\n",
    "    encoded = encoder(autoencoder_input)\n",
    "    decoded = decoder(encoded)\n",
    "    \n",
    "    autoencoder = tf.keras.Model(autoencoder_input, decoded, name=\"autoencoder\")\n",
    "    \n",
    "    # 5) Compile & train\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    result = autoencoder.fit(\n",
    "        x=training_data,\n",
    "        y=training_data,\n",
    "        epochs=200,\n",
    "        batch_size=16,\n",
    "        validation_data=(validate_data, validate_data),\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True)]\n",
    "    )\n",
    "\n",
    "\n",
    "    autoencoder.save(os.path.join(outdir, \"autoencoder_model\"), overwrite=True)\n",
    "    encoder.save(os.path.join(outdir, \"encoder_model\"), overwrite=True)\n",
    "    decoder.save(os.path.join(outdir, \"decoder_model\"), overwrite=True)\n",
    "\n",
    "    # Plot training & validation loss\n",
    "    plt.plot(result.history['loss'], label='Training Loss')\n",
    "    plt.plot(result.history['val_loss'], label='Validation Loss')\n",
    "    \n",
    "    plt.title('Autoencoder Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "# 6) Extract low-dimensional features\n",
    "train_latent = encoder.predict(training_data)\n",
    "val_latent = encoder.predict(validate_data)\n",
    "all_data = encoder.predict(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d448f3-44c6-4f17-a2a1-f3a0f0bb5303",
   "metadata": {},
   "source": [
    "# Predict which original features cluster\n",
    "\n",
    "Looking at the correlation to the encoded data\n",
    "\n",
    "We compute Correlation Between Each Original Feature and Each Latent Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7875e4-478c-4106-94b0-9af2516dfb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = df.shape[1]  # 1512\n",
    "num_latent_dims = all_data.shape[1]       # 50 - the number of output layers\n",
    "\n",
    "corr_matrix = pd.DataFrame(index=df.columns, columns=[f\"LD_{i}\" for i in range(num_latent_dims)])\n",
    "\n",
    "for i in range(num_features):\n",
    "    for j in range(num_latent_dims):\n",
    "        try:\n",
    "            corr = np.corrcoef(df.iloc[:, i], all_data[:, j])[0, 1]\n",
    "            corr_matrix.iloc[i, j] = corr\n",
    "        except:\n",
    "            print(f\"Error predicting {i} and {j}\", file=sys.stderr)\n",
    "            break\n",
    "\n",
    "corr_matrix = corr_matrix.astype(float)\n",
    "corr_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b584d17-5da4-4f10-a4be-0dc4a6b60653",
   "metadata": {},
   "source": [
    "## Cluster the original input data and the latent models using k-means\n",
    "\n",
    "Here, we take the correlation matrix from the ANN - correlating which of our 1512 features map to the same output nodes from the ANN and we build clusters using k-means hierarchical clustering.\n",
    "\n",
    "The 150 clusters is somewhat random - it is ~10% of the input data and it only gives 3 singletons, and breaks the Pseudomonas cluster down quite a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d39c923-d82a-4f38-af65-a6545cbdda81",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join(outdir, 'clusters.json')):\n",
    "    with open(os.path.join(outdir, 'clusters.json'), 'r') as file:\n",
    "        data = json.load(file)\n",
    "    tmpjsondf = pd.DataFrame(list(data.items()), columns=['Cluster', 'Feature'])\n",
    "    cluster_assignments = tmpjsondf.explode('Feature').reset_index(drop=True)\n",
    "    cluster_assignments['Cluster'] = cluster_assignments['Cluster'].astype(int)\n",
    "else:\n",
    "    # Calculate linkage on the rows (i.e., the features)\n",
    "    Z = linkage(corr_matrix, method='ward', metric='euclidean')\n",
    "    \n",
    "    k = 150  # more clusters, smaller groups!\n",
    "    clusters = fcluster(Z, k, criterion='maxclust')  # Each feature gets a cluster ID [1..k]\n",
    "    \n",
    "    # create a df with the cluster assignments\n",
    "    cluster_assignments = pd.DataFrame({\n",
    "        \"Feature\": corr_matrix.index,  # the feature names\n",
    "        \"Cluster\": clusters\n",
    "    })\n",
    "    cluster_assignments.sort_values(\"Cluster\", inplace=True)\n",
    "    grouped = cluster_assignments.groupby(\"Cluster\")\n",
    "    by_cluster = {str(cluster_id): group[\"Feature\"].tolist() for cluster_id, group in grouped}\n",
    "    with open(os.path.join(outdir, 'clusters.json'), 'w') as json_file:\n",
    "        json.dump(by_cluster, json_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2b7850-416e-4edd-84f4-2047f237afb6",
   "metadata": {},
   "source": [
    "### Print out the clusters that contain _Pseudomonas_ subsystems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b345b7-ae47-45b5-9d1e-ff07cd6ad2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_cluster = None\n",
    "interesting_cluster_count = 0\n",
    "grouped = cluster_assignments.groupby(\"Cluster\")\n",
    "for cluster_id, group in grouped:\n",
    "    if group['Feature'].str.contains('Pseudomonas').any():\n",
    "        if group['Feature'].str.contains('Pseudomonas').sum() > interesting_cluster_count:\n",
    "            interesting_cluster_count = group['Feature'].str.contains('Pseudomonas').sum()\n",
    "            interesting_cluster = cluster_id\n",
    "        print(f\"--- Cluster {cluster_id} : Length {group['Feature'].shape[0]} ---\")\n",
    "        print(group[\"Feature\"].tolist())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a8c9d3-f60c-47d2-9841-a39c773201d6",
   "metadata": {},
   "source": [
    "### How many singletons are there in the groups?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3131c0d-c795-4d9c-9a65-5a1f54c0d7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "singletons = 0\n",
    "for cluster_id, group in grouped:\n",
    "    if group['Feature'].shape[0] == 1:\n",
    "        singletons+=1\n",
    "print(f\"There are {singletons} single clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590071b1-f143-4c46-a4f2-1483ea74d52a",
   "metadata": {},
   "source": [
    "## Write all the clusters out\n",
    "\n",
    "This is so we can use them again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75b7ee3-235e-41d8-93f5-63b2d13f535e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = cluster_assignments.groupby(\"Cluster\")\n",
    "with open(os.path.join(outdir, 'cluster_features.txt'), 'w') as out:\n",
    "    for cluster_id, group in grouped:\n",
    "        print(f\"--- Cluster {cluster_id} : Length {group['Feature'].shape[0]} ---\", file=out)\n",
    "        print(group[\"Feature\"].tolist(), file=out)\n",
    "        print(file=out)\n",
    "cluster_assignments.to_csv(os.path.join(outdir, 'clusters.tsv'), sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d15222-f63e-4838-a527-4324986eb67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "intcols = ['Pseudomonas Culture', 'N12M_Pseudomonas aeruginosa', 'N12M_mucoid', 'N12M_non_mucoid']\n",
    "df_clust = df[cluster_assignments.loc[cluster_assignments[\"Cluster\"] == interesting_cluster, \"Feature\"]]\n",
    "merged_df_clust = df_clust.join(metadata[intcols])\n",
    "merged_df_clust.to_csv(os.path.join(outdir, 'Pseudomonas_cluster.tsv'), sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312fc5cb-65bc-46a9-9efa-0eef4c2f5d4b",
   "metadata": {},
   "source": [
    "### Cluster correlations\n",
    "\n",
    "Now we have clusters, lets take one cluster and see how it correlates to itself.\n",
    "\n",
    "Note that the ANN has learnt things that both positively and negatively correlate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d63451-a443-4c29-bbbf-cac28680d7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clust = df[cluster_assignments.loc[cluster_assignments[\"Cluster\"] == interesting_cluster, \"Feature\"]]\n",
    "df_clust_corr = df_clust.corr()\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.heatmap(df_clust_corr, annot=False, cmap='coolwarm')\n",
    "plt.title(f'Cluster {interesting_cluster} Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df25a57d-2755-482d-a1ad-d7a5b5cc26f8",
   "metadata": {},
   "source": [
    "## Creating a new df with principal components of the data\n",
    "\n",
    "Here, we make a new data frame that has the PC1 for each _cluster_. This doesn't matter if the cluster has positive/negative correlations because we are looking at the PC.\n",
    "\n",
    "We can use this matrix in our machine learning approaches like gradient boosted random forest!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb49793-b7b4-4cc4-acb8-923b061f6a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join(outdir, 'pc_df.tsv')):\n",
    "    pc_df = pd.read_csv(os.path.join(outdir, 'pc_df.tsv'), sep=\"\\t\", index_col=0)\n",
    "else:\n",
    "    pca = PCA(n_components=1)\n",
    "    pc_df = pd.DataFrame(index=df.index, columns=[f\"Cluster {x+1}\" for x in range(len(cluster_assignments.groupby(\"Cluster\").size()))])\n",
    "    for cluster_id, group in grouped:\n",
    "        df_clust = df[cluster_assignments.loc[cluster_assignments[\"Cluster\"] == cluster_id, \"Feature\"]]\n",
    "        pc_df[f\"Cluster {cluster_id}\"] = pca.fit_transform(df_clust).ravel()\n",
    "    pc_df.to_csv(os.path.join(outdir, 'pc_df.tsv'), sep=\"\\t\")\n",
    "pc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a9f139-7a81-4761-afb4-35ca56045e0e",
   "metadata": {},
   "source": [
    "### Plot a PCA\n",
    "\n",
    "We just plot the PCA of one component to see what it looks like. This is our `interesting component`, of course, the one with all the Pseudomonodaceae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a4c680-1a7c-4755-976a-bbcf85b3f1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "\n",
    "intcol = 'Pseudomonas Culture'\n",
    "df_clust = df[cluster_assignments.loc[cluster_assignments[\"Cluster\"] == interesting_cluster, \"Feature\"]]\n",
    "merged_df_clust = df_clust.join(metadata[[intcol]])\n",
    "\n",
    "pca_result = pca.fit_transform(df_clust)\n",
    "pca_df = pd.DataFrame(data=pca_result, index=df.index, columns=['PC1', 'PC2'])\n",
    "\n",
    "# Get loadings\n",
    "loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "loadings_df = pd.DataFrame(loadings, index=df_clust.columns, columns=['PC1', 'PC2'])\n",
    "\n",
    "# Create a DataFrame for top loadings\n",
    "top_loadings_df = loadings_df.loc[loadings_df['PC1'].abs().sort_values(ascending=False).index]\n",
    "top_loadings_df.head()\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_ * 100\n",
    "pc1_variance = explained_variance[0]\n",
    "pc2_variance = explained_variance[1]\n",
    "\n",
    "colours = np.where(merged_df_clust['Pseudomonas Culture'] == 1, 'blue', 'red')\n",
    "\n",
    "# Plot the PCA results\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(pca_df['PC1'], pca_df['PC2'], alpha=0.2, c=colours)\n",
    "plt.title(f\"Cluster {interesting_cluster}\")\n",
    "plt.xlabel(f'Principal Component 1 ({pc1_variance:.3f}%)')\n",
    "plt.ylabel(f'Principal Component 2 ({pc2_variance:.3f}%)')\n",
    "\n",
    "# add the loadings ... we only plot maxloadings here\n",
    "maxloadings = 5\n",
    "if len(loadings) < maxloadings:\n",
    "    maxloadings = len(loadings)\n",
    "\n",
    "plotscaler = 2\n",
    "texts = []\n",
    "colour_cycle = cycle(mcolors.TABLEAU_COLORS)\n",
    "found_pseudomonas = False\n",
    "for i in range(maxloadings):\n",
    "    c = next(colour_cycle)\n",
    "    xpos = top_loadings_df.iloc[i, 0]*plotscaler\n",
    "    ypos = top_loadings_df.iloc[i, 1]*plotscaler\n",
    "    plt.arrow(0, 0, xpos, ypos, \n",
    "              color=c, alpha=0.5, width=0.05)\n",
    "    texts.append(plt.text(xpos, ypos, top_loadings_df.index[i], color=c))\n",
    "\n",
    "adjust_text(texts)\n",
    "\n",
    "\n",
    "# Add a legend\n",
    "blue_patch = plt.Line2D([0], [0], marker='o', color='w', label='Pseudomonas culture positive', \n",
    "                         markerfacecolor='blue', alpha=0.2, markersize=10)\n",
    "red_patch = plt.Line2D([0], [0], marker='o', color='w', label='Pseudomonas culture negative', \n",
    "                        markerfacecolor='red', alpha=0.2, markersize=10)\n",
    "\n",
    "\n",
    "plt.legend(handles=[blue_patch, red_patch])\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660a34b1-905e-4e76-8523-5b3ee178460a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clust = df[cluster_assignments.loc[cluster_assignments[\"Cluster\"] == interesting_cluster, \"Feature\"]]\n",
    "merged_df_clust = df_clust.join(metadata[[intcol]])\n",
    "df_clust_m = merged_df_clust.melt(id_vars=intcol, var_name='Features', value_name='Normalised read abundance')\n",
    "df_clust_m = df_clust_m[df_clust_m['Normalised read abundance'] > 0]\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.stripplot(data=df_clust_m, x='Features', y='Normalised read abundance', hue=intcol, dodge=True, jitter=True)\n",
    "plt.xticks(rotation=90)\n",
    "# plt.ylim(0, 10000)\n",
    "plt.title(f\"Read abundance for cluster {interesting_cluster} with counts > 0\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8852a188-8700-44f4-ba2f-a7f63bc4552a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clust = df[cluster_assignments.loc[cluster_assignments[\"Cluster\"] == interesting_cluster, \"Feature\"]]\n",
    "merged_df_clust = df_clust.join(metadata[[intcol]])\n",
    "df_clust_m = merged_df_clust.melt(id_vars=intcol, var_name='Features', value_name='Normalised read abundance')\n",
    "df_clust_m = df_clust_m[df_clust_m['Normalised read abundance'] > 0]\n",
    "plt.figure(figsize=(12, 12))\n",
    "g = sns.stripplot(data=df_clust_m, x='Features', y='Normalised read abundance', hue=intcol, dodge=True, jitter=True)\n",
    "g.set(yscale=\"log\")\n",
    "g.set_ylabel('log(Normalised read abundance)')\n",
    "custom_labels = {0: 'No', 1: 'Yes'}\n",
    "handles, labels = g.get_legend_handles_labels()  # Get one set of handles and labels\n",
    "updated_labels = [custom_labels[float(label)] for label in labels]\n",
    "g.get_legend().remove()\n",
    "g.legend(handles, updated_labels, loc='upper center', ncol=2, title=intcol_title)\n",
    "plt.xticks(rotation=90)\n",
    "# plt.ylim(0, 10000)\n",
    "#plt.title(f\"Read abundance for cluster {interesting_cluster}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('final_images/cluster31_read_abundance.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923a83bc-136d-4f4b-9db6-034f2130e955",
   "metadata": {},
   "source": [
    "# GBRF\n",
    "\n",
    "This code is directly from the GBRF page, but we are using pc_df instead of df, and we're going to predict `Pseudomonas culture`\n",
    "\n",
    "When we run this multiple times, we get the same answer! Now we only need to run the GBRF once per metadata column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20c0da0-e824-4d3b-a079-1ab11e1b8b58",
   "metadata": {},
   "source": [
    "# Run the GBRF for every metadata sample\n",
    "\n",
    "This code is taken from the gradient_boosting.py script with some minor changes, but runs the GBRF once per sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a281576a-714a-4994-9a9a-050a7538fe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore adjustText warnings in this block\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='adjustText')\n",
    "\n",
    "replace_index = re.compile(r'^\\d+\\s+')\n",
    "replace_nonword = re.compile(r'\\W+')\n",
    "\n",
    "resultsfile = open(os.path.join(outdir, 'gbrf_results.txt'), 'w')\n",
    "print(f\"Predictor\\tFeature\\tImportance\", file=resultsfile)\n",
    "\n",
    "skip_columns = {'minion', 'MGI', 'pwCF_ID', 'Sample_Type', 'Corr', 'CF gene 1', 'CS_NTM_(Smear negative)', 'CS_Aspergillus niger', 'CS_Aspergillus terreus',\n",
    "                'CS_Scedosporium apiospermum', 'O_Scedosporium apiospermum', 'O_Trichosporon mycotoxinivorans', '3 Aztreonam_IV',\n",
    "                'DNA_extraction_ conc', 'SAGC ULN', 'DNA Conc. (ng/ul)',\n",
    "                'Index I7', 'Index I5', 'Mean_Size_BP', 'Total Clusters Passing Filter (Million)'}\n",
    "\n",
    "n_estimators=1000\n",
    "\n",
    "# make the image directories\n",
    "os.makedirs(os.path.join(outdir, \"img\", \"clusters\"), exist_ok=True)\n",
    "\n",
    "shouldskip = []\n",
    "doskip = True\n",
    "for intcol in metadata.columns:\n",
    "    if intcol == 'XXXXX': # replace this if you want to restart part way through!\n",
    "        doskip = False\n",
    "    if doskip:\n",
    "        continue\n",
    "    if intcol in skip_columns:\n",
    "        continue\n",
    "    print(f\"Working on {intcol}\", file=sys.stderr)\n",
    "\n",
    "    # set up our titles and the filename\n",
    "    intcol_title = replace_index.sub('', intcol)\n",
    "    intcol_filename = intcol.replace(\" \", \"_\")\n",
    "    intcol_filename = replace_nonword.sub('', intcol_filename)\n",
    "\n",
    "    merged_df = pc_df.join(metadata[[intcol]]).dropna(subset=intcol)\n",
    "\n",
    "    # do we need to encode this column\n",
    "    custom_labels = {0: 'No', 1: 'Yes'}\n",
    "    categorical_data = False\n",
    "    if pd.api.types.is_numeric_dtype(metadata[intcol]):\n",
    "        # this is an numeric column, so we can just continue\n",
    "        categorical_data = False\n",
    "    elif isinstance(metadata[intcol].dtype, pd.CategoricalDtype) and pd.api.types.is_numeric_dtype(metadata[intcol].cat.categories.dtype):\n",
    "        # this is a categorical column with numeric categories so we can also continue\n",
    "        categorical_data = True\n",
    "    elif isinstance(merged_df[intcol].dtype, pd.CategoricalDtype):\n",
    "        # this is a categorical column with string categories so we need to encode it\n",
    "        enc = OrdinalEncoder()\n",
    "        metadata_encoder = enc.fit(merged_df[[intcol]])\n",
    "        categories = metadata_encoder.categories_[0]\n",
    "        custom_labels = {code: cat for code, cat in enumerate(categories)}\n",
    "        merged_df[intcol] = metadata_encoder.transform(merged_df[[intcol]])\n",
    "        categorical_data = True\n",
    "    else:\n",
    "        # not sure what this is, so we skip it for now\n",
    "        print(f\"Error: {intcol} is not a numeric or categorical column. Skipped\", file=sys.stderr)\n",
    "        continue\n",
    "\n",
    "    X = merged_df.drop(intcol, axis=1)\n",
    "    y = merged_df[intcol]\n",
    "\n",
    "\n",
    "    met = None\n",
    "\n",
    "    if categorical_data or metadata[intcol].dtype == 'object':\n",
    "        try:\n",
    "            mse, feature_importances_sorted = cf_analysis_lib.gb_classifier(X, y, n_estimators)\n",
    "            met = 'classifier'\n",
    "        except ValueError as e:\n",
    "            shouldskip.append(intcol)\n",
    "            continue\n",
    "    else:\n",
    "        try:\n",
    "            mse, feature_importances_sorted = cf_analysis_lib.gb_regressor(X, y, n_estimators)\n",
    "            met = 'regressor'\n",
    "        except ValueError as e:\n",
    "            shouldskip.append(intcol)\n",
    "            continue\n",
    "\n",
    "    print(f\"Mean squared error for {intcol} using {met} is {mse}\", file=resultsfile)\n",
    "    \n",
    "    print(f\"Features appearing at least once\", file=resultsfile)\n",
    "    for x in feature_importances_sorted.index[:20]:\n",
    "        print(f\"{intcol}\\t{x}\\t{feature_importances_sorted.loc[x, 'importance']}\", file=resultsfile)\n",
    "    print(file=resultsfile)\n",
    "        \n",
    "    y_features = 10\n",
    "\n",
    "    topN = list(feature_importances_sorted[:y_features].index) + [intcol]\n",
    "    fig, axes = plt.subplots(figsize=(10, 6), nrows=1, ncols=2, sharey='row', sharex='col')\n",
    "    cf_analysis_lib.plot_feature_importance(axes[0], feature_importances_sorted[:y_features][::-1], \"\")\n",
    "    cf_analysis_lib.plot_feature_abundance(axes[1], merged_df[topN][::-1], intcol, intcol_title)\n",
    "\n",
    "\n",
    "    handles, labels = axes[1].get_legend_handles_labels()  # Get one set of handles and labels\n",
    "    updated_labels = labels\n",
    "    try:\n",
    "        updated_labels = [custom_labels[float(label)] for label in labels]\n",
    "    except Exception as e:\n",
    "        print(f\"Couldn't use float for labels {e}.\", file=sys.stderr)\n",
    "\n",
    "    for ax in axes.flat:\n",
    "        if ax.get_legend() is not None:  # Check if legend exists\n",
    "            ax.get_legend().remove()\n",
    "\n",
    "    plt.xticks(rotation=90)\n",
    "    fig.legend(handles, updated_labels, loc='upper center', ncol=2, title=intcol_title)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.9])\n",
    "    plt.savefig(os.path.join(outdir, \"img\", f\"{intcol_filename}_importance_abundance.png\"))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    for interesting_cluster in feature_importances_sorted[:y_features].index:\n",
    "        if cluster_assignments[cluster_assignments[\"Cluster\"] == int_cluster_num].shape[0] < 2:\n",
    "            continue\n",
    "        interesting_cluster_filename = interesting_cluster.replace(\" \", \"_\")\n",
    "        int_cluster_num = int(interesting_cluster.replace(\"Cluster \", \"\"))\n",
    "        fig, axes = plt.subplots(figsize=(8, 11), nrows=2, ncols=1)\n",
    "        cf_analysis_lib.plot_pca(axes[0], df, metadata, cluster_assignments, int_cluster_num, intcol)\n",
    "        cf_analysis_lib.plot_abundance_stripplot(axes[1], df, metadata, cluster_assignments, int_cluster_num, intcol)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(outdir, \"img\", \"clusters\", f\"{intcol_filename}_{interesting_cluster_filename}.png\"))\n",
    "        plt.close()\n",
    "    \n",
    "\n",
    "resultsfile.close()\n",
    "\n",
    "if shouldskip:\n",
    "    print(f\"THESE MODELS FAILED. We skipped them, and you should!\\n{shouldskip}\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a64cc98-f4b2-470e-af82-42d462111481",
   "metadata": {},
   "source": [
    "# Random Forest for a Single Column\n",
    "\n",
    "This repeats the RF for one column, but doesn't save any files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f732f5e6-cdd8-4cd6-9fdf-c73eb10e603e",
   "metadata": {},
   "outputs": [],
   "source": [
    "intcol = 'CS_Aspergillus fumigatus'\n",
    "print(f\"'{intcol}' has samples from {metadata.groupby(intcol).size()[1]} pwCF\")\n",
    "\n",
    "# ignore adjustText warnings in this block\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='adjustText')\n",
    "\n",
    "replace_index = re.compile(r'^\\d+\\s+')\n",
    "replace_nonword = re.compile(r'\\W+')\n",
    "\n",
    "\n",
    "n_estimators=1000\n",
    "\n",
    "# set up our titles and the filename\n",
    "intcol_title = replace_index.sub('', intcol)\n",
    "intcol_filename = intcol.replace(\" \", \"_\")\n",
    "\n",
    "merged_df = pc_df.join(metadata[[intcol]]).dropna(subset=intcol)\n",
    "\n",
    "# do we need to encode this column\n",
    "custom_labels = {0: 'No', 1: 'Yes'}\n",
    "categorical_data = False\n",
    "if pd.api.types.is_numeric_dtype(metadata[intcol]):\n",
    "    # this is an numeric column, so we can just continue\n",
    "    categorical_data = False\n",
    "elif isinstance(metadata[intcol].dtype, pd.CategoricalDtype) and pd.api.types.is_numeric_dtype(metadata[intcol].cat.categories.dtype):\n",
    "    # this is a categorical column with numeric categories so we can also continue\n",
    "    categorical_data = True\n",
    "elif isinstance(merged_df[intcol].dtype, pd.CategoricalDtype):\n",
    "    # this is a categorical column with string categories so we need to encode it\n",
    "    enc = OrdinalEncoder()\n",
    "    metadata_encoder = enc.fit(merged_df[[intcol]])\n",
    "    categories = metadata_encoder.categories_[0]\n",
    "    custom_labels = {code: cat for code, cat in enumerate(categories)}\n",
    "    merged_df[intcol] = metadata_encoder.transform(merged_df[[intcol]])\n",
    "    categorical_data = True\n",
    "else:\n",
    "    # not sure what this is, so we skip it for now\n",
    "    print(f\"Error: {intcol} is not a numeric or categorical column. Skipped\", file=sys.stderr)\n",
    "\n",
    "\n",
    "X = merged_df.drop(intcol, axis=1)\n",
    "y = merged_df[intcol]\n",
    "\n",
    "\n",
    "met = None\n",
    "\n",
    "if categorical_data or metadata[intcol].dtype == 'object':\n",
    "    mse, feature_importances_sorted = cf_analysis_lib.gb_classifier(X, y, n_estimators)\n",
    "    met = 'classifier'\n",
    "else:\n",
    "    mse, feature_importances_sorted = cf_analysis_lib.gb_regressor(X, y, n_estimators)\n",
    "    met = 'regressor'\n",
    "\n",
    "\n",
    "print(f\"Mean squared error for {intcol} using {met} is {mse}\", file=sys.stderr)\n",
    "\n",
    "    \n",
    "y_features = 10\n",
    "\n",
    "topN = list(feature_importances_sorted[:y_features].index) + [intcol]\n",
    "fig, axes = plt.subplots(figsize=(10, 6), nrows=1, ncols=2, sharey='row', sharex='col')\n",
    "cf_analysis_lib.plot_feature_importance(axes[0], feature_importances_sorted[:y_features][::-1], \"\")\n",
    "cf_analysis_lib.plot_feature_abundance(axes[1], merged_df[topN][::-1], intcol, intcol_title)\n",
    "\n",
    "\n",
    "handles, labels = axes[1].get_legend_handles_labels()  # Get one set of handles and labels\n",
    "updated_labels = labels\n",
    "try:\n",
    "    updated_labels = [custom_labels[float(label)] for label in labels]\n",
    "except Exception as e:\n",
    "    print(f\"Couldn't use float for labels {e}.\", file=sys.stderr)\n",
    "\n",
    "for ax in axes.flat:\n",
    "    if ax.get_legend() is not None:  # Check if legend exists\n",
    "        ax.get_legend().remove()\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "fig.legend(handles, updated_labels, loc='upper center', ncol=2, title=intcol_title)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.9])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6f7b70-0132-4b58-951d-73dc3702e39b",
   "metadata": {},
   "source": [
    "# Print some information about an interesting cluster or column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073f52c2-3e65-4d9b-a977-2a9d110c40e3",
   "metadata": {},
   "source": [
    "# Manually make an image. \n",
    "\n",
    "Note that this recreates the images we make automatically, but allows you to tweak things. It saves them to a new location!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e8599a-992d-4ee6-8986-57e5674a3b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "intcol = 'CS_Aspergillus fumigatus'\n",
    "interesting_cluster = 71\n",
    "print(f\"'{intcol}' has samples from {metadata.groupby(intcol).size()[1]} pwCF\")\n",
    "\n",
    "if isinstance(interesting_cluster, int):\n",
    "    interesting_cluster_filename = f\"Cluster_{interesting_cluster}\"\n",
    "elif 'Cluster' in interesting_cluster:\n",
    "    interesting_cluster_filename = interesting_cluster.replace(\" \", \"_\")\n",
    "else:\n",
    "    interesting_cluster_filename = f\"Cluster_{interesting_cluster}\"\n",
    "intcol_filename = intcol.replace(\" \", \"_\")\n",
    "intcol_filename = replace_nonword.sub('', intcol_filename)\n",
    "\n",
    "\n",
    "# make a new dir so we don't overwrite!\n",
    "os.makedirs(os.path.join(outdir, \"img\", \"clusters_manual\"), exist_ok=True)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "int_cluster_num = interesting_cluster\n",
    "\n",
    "df_clust = df[cluster_assignments.loc[cluster_assignments[\"Cluster\"] == int_cluster_num, \"Feature\"]]\n",
    "merged_df_clust = df_clust.join(metadata[[intcol]])\n",
    "\n",
    "if intcol == 'CFLD':\n",
    "    mask = merged_df_clust[intcol].notna() & (merged_df_clust[intcol] != 'Unknown')\n",
    "    merged_df_clust = merged_df_clust[mask]\n",
    "    merged_df_clust[intcol] = merged_df_clust[intcol].cat.remove_unused_categories()\n",
    "    df_clust = merged_df_clust.drop(intcol, axis=1)\n",
    "\n",
    "pca_result = pca.fit_transform(df_clust)\n",
    "pca_df = pd.DataFrame(data=pca_result, index=df_clust.index, columns=['PC1', 'PC2'])\n",
    "\n",
    "# Get loadings\n",
    "loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "loadings_df = pd.DataFrame(loadings, index=df_clust.columns, columns=['PC1', 'PC2'])\n",
    "\n",
    "# Create a DataFrame for top loadings\n",
    "top_loadings_df = loadings_df.loc[loadings_df['PC1'].abs().sort_values(ascending=False).index]\n",
    "top_loadings_df.head()\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_ * 100\n",
    "pc1_variance = explained_variance[0]\n",
    "pc2_variance = explained_variance[1]\n",
    "\n",
    "# don't forget to change the legend\n",
    "intcol_neg = 0\n",
    "\n",
    "colours = np.where(merged_df_clust[intcol] == intcol_neg, 'blue', 'red')\n",
    "\n",
    "# Plot the PCA results\n",
    "fig, axes = plt.subplots(figsize=(8, 11), nrows=2, ncols=1)\n",
    "ax = axes[0]\n",
    "sns.scatterplot(data=pca_df, x='PC1', y='PC2', alpha=0.2, c=colours, ax=ax)\n",
    "ax.set_title(f\"Cluster {interesting_cluster}\")\n",
    "ax.set_xlabel(f'Principal Component 1 ({pc1_variance:.3f}%)')\n",
    "ax.set_ylabel(f'Principal Component 2 ({pc2_variance:.3f}%)')\n",
    "\n",
    "# add the loadings ... we only plot maxloadings here\n",
    "maxloadings = 5\n",
    "if len(loadings) < maxloadings:\n",
    "    maxloadings = len(loadings)\n",
    "\n",
    "plotscaler = 2\n",
    "texts = []\n",
    "colour_cycle = cycle(mcolors.TABLEAU_COLORS)\n",
    "\n",
    "\n",
    "for i in range(maxloadings):\n",
    "    c = next(colour_cycle)\n",
    "    xpos = top_loadings_df.iloc[i, 0]*plotscaler\n",
    "    ypos = top_loadings_df.iloc[i, 1]*plotscaler\n",
    "    ax.arrow(0, 0, xpos, ypos, \n",
    "              color=c, alpha=0.5, width=0.05)\n",
    "    loading_text = top_loadings_df.index[i]\n",
    "    if len(loading_text) > 30:\n",
    "        loading_text = loading_text[:30] + \"...\"\n",
    "    texts.append(ax.text(xpos, ypos, loading_text, color=c))\n",
    "\n",
    "adjust_text(texts, ax=ax)\n",
    "\n",
    "# Add a legend\n",
    "blue_patch = plt.Line2D([0], [0], marker='o', color='w', label=f'{intcol} Negative', \n",
    "                         markerfacecolor='blue', alpha=0.2, markersize=10)\n",
    "red_patch = plt.Line2D([0], [0], marker='o', color='w', label=f'{intcol} Positive', \n",
    "                        markerfacecolor='red', alpha=0.2, markersize=10)\n",
    "ax.legend(handles=[blue_patch, red_patch])\n",
    "\n",
    "## Read abundance\n",
    "ax = axes[1]\n",
    "\n",
    "df_clust_m = merged_df_clust.melt(id_vars=intcol, var_name='Features', value_name='Normalised read abundance')\n",
    "df_clust_m = df_clust_m[df_clust_m['Normalised read abundance'] > 0]\n",
    "\n",
    "cl = {'IP': 'red', 'OP': 'blue'}\n",
    "sns.stripplot(data=df_clust_m, x='Features', y='Normalised read abundance', hue=intcol, dodge=True, jitter=True, ax=ax)\n",
    "\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "for label in ax.get_xticklabels():\n",
    "    label.set_horizontalalignment('right')\n",
    "ax.set_title(f\"Read abundance for {intcol} cluster {interesting_cluster}\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(outdir, \"img\", \"clusters_manual\", f\"{intcol_filename}_{interesting_cluster_filename}.png\"))\n",
    "print(f'Wrote the new image to {os.path.join(outdir, \"img\", \"clusters_manual\", f\"{intcol_filename}_{interesting_cluster_filename}.png\")}', file=sys.stderr)\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9143d3-6862-48c3-9dcf-2270f684320d",
   "metadata": {},
   "source": [
    "# Plot some non-categorical data!\n",
    "\n",
    "Some of our data is continuous. Eeek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69361357-2ce7-4850-925c-e84bb29c0878",
   "metadata": {},
   "source": [
    "### Plot using the PCA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35636f6-65b7-4c27-99d4-53e4c5eb752f",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_pca = metadata[[intcol]].join(pca_df[['PC1']])\n",
    "plt.subplots(figsize=(8, 11))\n",
    "ax = sns.regplot(data=meta_pca, x=intcol, y='PC1',\n",
    "    scatter=True,         # Plot the scatter points\n",
    "    fit_reg=True,         # Fit and plot the regression line\n",
    "    ci=95,                # Shaded 95% confidence interval\n",
    "    scatter_kws={'color': 'blue', 'alpha': 0.2},  # Customize scatter points\n",
    "    line_kws={'color': 'red'}  # Customize regression line\n",
    ")\n",
    "\n",
    "ax.set_title(f\"Read abundance for {intcol} cluster {interesting_cluster}\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(os.path.join(outdir, \"img\", \"clusters_manual\", f\"{intcol_filename}_{interesting_cluster_filename}_regplot.png\"))\n",
    "print(f'Wrote the new image to {os.path.join(outdir, \"img\", \"clusters_manual\", f\"{intcol_filename}_{interesting_cluster_filename}_regplot.png\")}', file=sys.stderr)\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook-gpu [conda env:miniconda3-notebook_pytorch]",
   "language": "python",
   "name": "conda-env-miniconda3-notebook_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
